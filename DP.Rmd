---
title: "Dirichlet Process Prior"
author: "Kazuki Yoshida"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: html_document
---

```{r, message = FALSE, tidy = FALSE, echo = F}
## knitr configuration: http://yihui.name/knitr/options#chunk_options
library(knitr)
showMessage <- FALSE
showWarning <- TRUE
set_alias(w = "fig.width", h = "fig.height", res = "results")
opts_chunk$set(comment = "##", error= TRUE, warning = showWarning, message = showMessage,
               tidy = FALSE, cache = F, echo = T,
               fig.width = 7, fig.height = 7, dev.args = list(family = "sans"))
## for rgl
## knit_hooks$set(rgl = hook_rgl, webgl = hook_webgl)
## for animation
opts_knit$set(animation.fun = hook_ffmpeg_html)
## R configuration
options(width = 116, scipen = 5)
## Record start time
start_time <- Sys.time()
## Configure parallelization
## Parallel backend for foreach (also loads foreach and parallel; includes doMC)
library(doParallel)
## Reproducible parallelization
library(doRNG)
## Detect core count (Do not use on clusters)
n_cores <- parallel::detectCores()
## Used by parallel::mclapply() as default
options(mc.cores = n_cores)
## Used by doParallel as default
options(cores = n_cores)
## Register doParallel as the parallel backend for foreach
## http://stackoverflow.com/questions/28989855/the-difference-between-domc-and-doparallel-in-r
doParallel::registerDoParallel(cores = n_cores)
```

## References
- Books
  - [(BNPDA) Bayesian Nonparametric Data Analysis](https://www.springer.com/us/book/9783319189673)
  - [(FNBI) Fundamentals of Nonparametric Bayesian Inference](https://www.cambridge.org/core/books/fundamentals-of-nonparametric-bayesian-inference/C96325101025D308C9F31F4470DEA2E8)
- Software
  - [BNPDA code](https://web.ma.utexas.edu/users/pmueller/bnp/)
  - [CRAN DPpackage: Bayesian Nonparametric Modeling in R](https://cran.r-project.org/package=DPpackage)
  - [J Stat Softw. 2011. DPpackage: Bayesian Non- and Semi-parametric Modelling in R.](https://www.jstatsoft.org/article/view/v040i05)
- Papers
  - [Guindani (2014). A Bayesian Semi-parametric Approach for the Differential Analysis of Sequence Counts Data.](https://www.ncbi.nlm.nih.gov/pubmed/24833809)


## Load packages

```{r}
library(tidyverse)
library(DPpackage)
library(MCMCpack)
```

## Dirichlet Process
Here we will use the T cell receptor example in BNPDA (p10) for demonstration of Dirichlet process prior.

### Data
The data is based on Guindani 2014. Note count 0 does not exist in the data by design. Counts equal to and greater than 5 were not observed.

```{r}
tcell <- tribble(
    ~count, ~frequency,
    1, 37,
    2, 11,
    3, 5,
    4, 2,
    ## >= 5, 0,
    )
tcell
```

### Model
The model is the following.

$$
y_{i} | G \overset{iid}\sim G\\
G \sim DP(M, G_{0})
$$

The notation $y_{i} | G \overset{iid}\sim G$ puzzled me for a while, but the LHS data | parameter notation is necessary because we consider the joint distribution of data and parameter in Bayesian statistics. An iid argument like seen in Frequentist paradigm is only possible after fixing (conditioning on) the parameter. Importantly, once we marginalize this conditioning over the prior distribution, exchangeability remains, but not generally iid.

In the Dirichlet process formulation, each random probability measure $G$ retains the same support as the centering measure $G_{0}$. Here we will choose a Poisson with mean 2 that truncates at 1 (zeros are removed). Because we have a discrete centering measure, the partitioning property is relatively clear. If we collapse all values beyond 8 to 8+ bin, the following is the distribution.

$$
\begin{bmatrix}
   G(0)\\
   G(1)\\
   \vdots\\
   G(8+)\\
 \end{bmatrix}
\sim Dirichlet
\begin{bmatrix}
   M G_{0}(0)\\
   M G_{0}(1)\\
   \vdots\\
   M G_{0}(8+)\\
 \end{bmatrix}
$$


### Prior

Now we define a function to give appropriately collapsed finite probability vector for the centering measure.

```{r}
G0_vector <- function(lambda, min, max) {
    ## values below min are truncated
    ## values above max are collapsed

    ## Probabilities from Poisson(lambda) for min:max
    p_vec <- dpois(x = min:max, lambda = lambda)
    ## Probability for max+1 ...
    p_upper_tail <- 1 - ppois(q = max, lambda = lambda)
    ## Collapse to max
    p_vec[length(p_vec)] <- p_vec[length(p_vec)] + p_upper_tail
    ## Renormalize
    p_vec <- p_vec / sum(p_vec)
    ## Name
    names(p_vec) <- min:max
    ##
    return(p_vec)
}

## Collapse all values beyond 8 to 8+ bin
G0_vector(lambda = 2, min = 1, max = 8)
```

Now we can define a function to create random Dirichlet draws given the probability vector and mass $M$.

```{r}
## This function gives random Dirichlet draws via normalization of Gammas.
MCMCpack::rdirichlet

## Make sure G0_vector has integer names
draw_dirichlet <- function(n_draws, M, G0_vector) {
    MCMCpack::rdirichlet(n_draws, alpha = M * G0_vector) %>%
    t %>%
    as_data_frame %>%
    mutate(y = names(G0_vector)) %>%
    gather(key = .iter, value = p, -y) %>%
    mutate(.iter = as.integer(gsub("V", "", .iter)),
           M = M)
}

draw_dirichlet(n = 10, M = 1, G0_vector = G0_vector(lambda = 2, min = 1, max = 8))
```

Let us visualize some random draws from the prior at different $M$.

```{r}
G0_vector_values <- G0_vector(lambda = 2, min = 1, max = 8)

prior_mean <- data_frame(y = names(G0_vector_values),
                         p = G0_vector_values,
                         .iter = 0)

n_draws <- 50
prior_draws <- bind_rows(
    draw_dirichlet(n_draws = n_draws, M = 0.1, G0_vector = G0_vector_values),
    draw_dirichlet(n_draws = n_draws, M = 1,   G0_vector = G0_vector_values),
    draw_dirichlet(n_draws = n_draws, M = 10,  G0_vector = G0_vector_values),
    draw_dirichlet(n_draws = n_draws, M = 100, G0_vector = G0_vector_values),
    )

prior_draws %>%
    ggplot(mapping = aes(x = y, y = p, group = .iter)) +
    geom_line(data = prior_mean, size = 1) +
    geom_line(size = 0.1) +
    facet_wrap(~ M) +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
          legend.key = element_blank(),
          plot.title = element_text(hjust = 0.5),
          strip.background = element_blank())

```

The thicker line is $G_{0}$. We can see as the mass parameter $M$ increases, the prior distribution of $G$ (thin lines represent draws) is more tightly packed around $G_{0}$.


### Posterior inference

Now we can update our model with the observed data.

```{r}
df1 <- data_frame(y = c(1:4),
                  count = c(37, 11, 5, 2)) %>%
    mutate(total_n = sum(count))
df1
```

The total sample size is n = 55. Value 1 was observed most frequently. Values equal to and greater than 5 were not observed. The empirical distribution is the following.

```{r}
df1_extended <-  df1 %>%
    bind_rows(data_frame(y = 5:8,
                         count = 0)) %>%
    mutate(total_n = sum(count),
           p = count/total_n,
           .iter = 0)

df1_extended %>%
    ggplot(mapping = aes(x = y, y = p)) +
    geom_line() +
    theme_bw() +
    scale_y_continuous(limits = c(0,1)) +
    scale_x_discrete(limits = c(1:8)) +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
          legend.key = element_blank(),
          plot.title = element_text(hjust = 0.5),
          strip.background = element_blank())
```

A Dirichlet process prior conjugate prior to iid sampling, so we get a Dirichlet process posterior.

$$
G | \mathbf{y} \sim DP \left( M+n, \frac{M}{M+n}G_{0} + \frac{n}{M+n}\widehat{G} \right)
$$

The updated centering measure is a weighted mean of the prior centering measure $G_{0}$ and the empirical measure (empirical probability mass function) $\widehat{G}$. The mass (concentration) parameter becomes $M+n$. $M$ can roughly be interpreted as the "sample size" of the prior information.

```{r}
df1_posterior <- data_frame(y = as.integer(names(G0_vector_values)),
                            G0 = G0_vector_values,
                            n = 55,
                            G_hat = c(37, 11, 5, 2, 0, 0, 0, 0) / 55,
                            ## Weighted average at different M
                            `M = 0.1` = (0.1 * G0 + n * G_hat) / (0.1 + n),
                            `M = 1` = (1 * G0 + n * G_hat) / (1 + n),
                            `M = 10` = (10 * G0 + n * G_hat) / (10 + n),
                            `M = 100` = (100 * G0 + n * G_hat) / (100 + n),
                            )
G0_vector_new_M0.1 <- df1_posterior$`M = 0.1`
G0_vector_new_M1 <- df1_posterior$`M = 1`
G0_vector_new_M10 <- df1_posterior$`M = 10`
G0_vector_new_M100 <- df1_posterior$`M = 100`
names(G0_vector_new_M0.1) <- 1:8
names(G0_vector_new_M1) <- 1:8
names(G0_vector_new_M10) <- 1:8
names(G0_vector_new_M100) <- 1:8
df1_posterior
```


```{r}
n <- 55
n_draws <- 50
posterior_draws <- bind_rows(
    draw_dirichlet(n_draws = n_draws, M = n + 0.1, G0_vector = G0_vector_new_M0.1) %>% mutate(M = 0.1),
    draw_dirichlet(n_draws = n_draws, M = n + 1,   G0_vector = G0_vector_new_M1) %>% mutate(M = 1),
    draw_dirichlet(n_draws = n_draws, M = n + 10,  G0_vector = G0_vector_new_M10) %>% mutate(M = 10),
    draw_dirichlet(n_draws = n_draws, M = n + 100, G0_vector = G0_vector_new_M100) %>% mutate(M = 100),
    )

posterior_draws %>%
    ggplot(mapping = aes(x = y, y = p, group = .iter)) +
    ## Draws
    geom_line(size = 0.1) +
    ## Prior centering measure
    geom_line(data = prior_mean, size = 1) +
    ## Empirical distribution
    geom_line(data = df1_extended, size = 1, color = "red") +
    scale_y_continuous(limits = c(0,1)) +
    facet_wrap(~ M) +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5),
          legend.key = element_blank(),
          plot.title = element_text(hjust = 0.5),
          strip.background = element_blank())
```

Here the thick red line is the empirical measure $\widehat{G}$ and the thick black line is the prior centering measure $G_{0}$. The thin lines are the posterior samples. As $M$ increases, the posterior draws approach $G_{0}$ (more influence of the prior).


--------------------
- Top Page: http://rpubs.com/kaz_yos/
- Github: https://github.com/kaz-yos

```{r}
print(sessionInfo())
## Record execution time and multicore use
end_time <- Sys.time()
diff_time <- difftime(end_time, start_time, units = "auto")
cat("Started  ", as.character(start_time), "\n",
    "Finished ", as.character(end_time), "\n",
    "Time difference of ", diff_time, " ", attr(diff_time, "units"), "\n",
    "Used ", foreach::getDoParWorkers(), " cores\n",
    "Used ", foreach::getDoParName(), " as backend\n",
    sep = "")
```